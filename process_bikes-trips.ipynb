{"cells": [{"cell_type": "code", "execution_count": 1, "id": "451fb424", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/04/14 18:09:10 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n24/04/14 18:09:10 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n24/04/14 18:09:10 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n24/04/14 18:09:10 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\nimport pandas as pd\nimport pyarrow as parrow\nimport pyarrow.dataset as ds\nimport pyarrow.parquet as pq\nfrom pyspark.sql import types\nfrom pyspark.sql.functions import year\nfrom google.cloud import bigquery\n\n# BigQuery client setup\nclient = bigquery.Client()\ncurrent_project_id = client.project\n\n# Initialize a SparkSession\nspark = SparkSession.builder \\\n            .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar') \\\n            .getOrCreate()\n\n# Read the CSV file into a DataFrame\nschema = types.StructType([\n    types.StructField(\"departure\", types.TimestampType(), True),\n    types.StructField(\"return\", types.TimestampType(), True),\n    types.StructField(\"departure_id\", types.IntegerType(), True),\n    types.StructField(\"departure_name\", types.StringType(), True),\n    types.StructField(\"return_id\", types.IntegerType(), True),\n    types.StructField(\"return_name\", types.StringType(), True),\n    types.StructField(\"distance (m)\", types.DoubleType(), True),\n    types.StructField(\"duration (sec.)\", types.DoubleType(), True),\n    types.StructField(\"avg_speed (km/h)\", types.DoubleType(), True),\n    types.StructField(\"departure_latitude\", types.DoubleType(), True),\n    types.StructField(\"departure_longitude\", types.DoubleType(), True),\n    types.StructField(\"return_latitude\", types.DoubleType(), True),\n    types.StructField(\"return_longitude\", types.DoubleType(), True),\n    types.StructField(\"Air temperature (degC)\", types.DoubleType(), True)\n    ])\n    \nraw = spark.read \\\n    .format(\"csv\") \\\n    .option(\"compression\", \"gzip\") \\\n    .option(\"header\", True) \\\n    .option(\"inferSchema\", True)\\\n    .schema(schema)\\\n    .load(\"gs://pfcllotsb7jsqqvbjrnw3s-datasets/bike-rides-data.tar.gz\")\n\n"}, {"cell_type": "code", "execution_count": 2, "id": "38a6d910-bf2b-483b-af95-9243beba8c77", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- departure: timestamp (nullable = true)\n |-- return: timestamp (nullable = true)\n |-- departure_id: integer (nullable = true)\n |-- departure_name: string (nullable = true)\n |-- return_id: integer (nullable = true)\n |-- return_name: string (nullable = true)\n |-- distance: double (nullable = true)\n |-- duration_sec: double (nullable = true)\n |-- avg_speed_km_h: double (nullable = true)\n |-- departure_latitude: double (nullable = true)\n |-- departure_longitude: double (nullable = true)\n |-- return_latitude: double (nullable = true)\n |-- return_longitude: double (nullable = true)\n |-- air_temp_celcius: double (nullable = true)\n |-- year: integer (nullable = true)\n\n"}], "source": "# clean up column names, partition by year\nraw = raw.withColumnRenamed(\"distance (m)\", \"distance\")\\\n        .withColumnRenamed(\"duration (sec.)\", \"duration_sec\")\\\n        .withColumnRenamed(\"avg_speed (km/h)\", \"avg_speed_km_h\")\\\n        .withColumnRenamed(\"Air temperature (degC)\", \"air_temp_celcius\")\\\n        .withColumn(\"year\", year(raw[\"departure\"]))\n\nraw.printSchema()"}, {"cell_type": "code", "execution_count": 8, "id": "f1f61c30-4efa-482b-b568-c75291389ef9", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "departure points num: 347\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+------------+----------------+-------------------+------------------+\n|departure_id|  departure_name|departure_longitude|departure_latitude|\n+------------+----------------+-------------------+------------------+\n|         631|  Friisil\u00e4naukio|          24.721367|         60.162842|\n|         619|    Etuniementie|          24.724835|         60.160645|\n|         617|   Tiistinkallio|          24.729112|         60.153737|\n|         637|     Ruomelantie|          24.729314|         60.174524|\n|         641|    Komeetankatu|          24.735969|         60.167216|\n|         627|    Piispansilta|          24.738416|         60.162403|\n|         609|  Sepetlahdentie|          24.741234|         60.152343|\n|         901|         Outotec|          24.742219|         60.163412|\n|         653|        Lukutori|          24.742936|         60.187086|\n|         645|   Piispankallio|          24.743532|         60.168441|\n|         613|   Matinkyl\u00e4ntie|          24.744978|         60.155689|\n|         647|       Lystim\u00e4ki|          24.745666|         60.172814|\n|         623|      Nelikkotie|           24.74663|         60.161588|\n|         601|         Nokkala|          24.754344|         60.147974|\n|         577|     Olarinluoma|          24.760606|         60.180848|\n|         581| Niittykumpu (M)|          24.763184|         60.170758|\n|         587|     Hauenkallio|          24.767831|         60.162477|\n|         589|Haukilahdenranta|          24.769864|         60.158048|\n|         900|      Orionintie|          24.771826|         60.180118|\n|         579|       Niittymaa|          24.775913|         60.171127|\n+------------+----------------+-------------------+------------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "## get a table with all departure stations, removing duplicates\nfrom pyspark.sql.functions import row_number, lit, col\nfrom pyspark.sql.window import Window\n\nwindow = Window.partitionBy(\"departure_longitude\", \"departure_latitude\").orderBy(lit('A'))\n\ndeparture_points = raw.filter(raw.departure_id.isNotNull())\\\n                    .select(\"departure_id\", \"departure_name\", \"departure_longitude\", \"departure_latitude\")\\\n                    .distinct()\\\n                    .withColumn(\"row\", row_number().over(window))\\\n                    .filter(col(\"row\") == 1)\\\n                    .drop(\"row\")\n\nprint(f\"departure points num: {departure_points.count()}\")\ndeparture_points.show()\n\n# save departure stations data into the datalake\ntable_path = f'gs://{current_project_id}-datalake/helinski-bike-trips/stations'\ndeparture_points.write.mode(\"overwrite\").parquet(table_path)"}, {"cell_type": "code", "execution_count": 9, "id": "ed2a5d6b-2722-4947-a81f-779ea6923f18", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- departure: timestamp (nullable = true)\n |-- return: timestamp (nullable = true)\n |-- departure_id: integer (nullable = true)\n |-- departure_name: string (nullable = true)\n |-- return_id: integer (nullable = true)\n |-- return_name: string (nullable = true)\n |-- distance: double (nullable = true)\n |-- duration_sec: double (nullable = true)\n |-- avg_speed_km_h: double (nullable = true)\n |-- departure_latitude: double (nullable = true)\n |-- departure_longitude: double (nullable = true)\n |-- return_latitude: double (nullable = true)\n |-- return_longitude: double (nullable = true)\n |-- air_temp_celcius: double (nullable = true)\n |-- year: integer (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 173:======================================>                  (2 + 1) / 3]\r"}, {"name": "stdout", "output_type": "stream", "text": "raw data count: 12157460 / cleaned data count: 12157458\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#some years are missing departure id, so, we need to fill those using departure coordinates if possible\n#Also some stations have multiple id's for the same name/coordinates, we should unify id's per coordinate\nraw.createOrReplaceTempView('raw')\ndeparture_points.createOrReplaceTempView('dept_points')\n\n# cc = raw.filter(raw.departure_longitude.isNotNull()).count()\n# print(f\"raw rows with coordinates {cc}\")\n\ncleaned = spark.sql(\"\"\"\n                      SELECT raw.departure, raw.return, dept_points.departure_id as departure_id, raw.departure_name,raw.return_id, raw.return_name, \n                      raw.distance, raw.duration_sec, raw.avg_speed_km_h, raw.departure_latitude, \n                      raw.departure_longitude, raw.return_latitude, raw.return_longitude, \n                      raw.air_temp_celcius, raw.year\n                      FROM raw\n                      LEFT JOIN dept_points \n                      ON raw.departure_longitude = dept_points.departure_longitude AND raw.departure_latitude = dept_points.departure_latitude\n                      \"\"\")\\\n                .filter(col(\"departure_id\").isNotNull())\n\ncleaned.printSchema()\nprint(f\"raw data count: {raw.count()} / cleaned data count: {cleaned.count()}\")"}, {"cell_type": "code", "execution_count": 10, "id": "a3b8c377", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# save clean data into datalake\nroot_path = f'gs://{current_project_id}-datalake/helinski-bike-trips'\ncleaned.write.partitionBy(\"year\").mode(\"overwrite\").parquet(root_path)\n"}, {"cell_type": "code", "execution_count": 12, "id": "5f7ca7ec", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#############################################################################################\n# Write the data to a bigQuery table, partitioned by year, clustered by departure_id\n#############################################################################################\n# Specify your dataset and table\ndataset_id = 'zoomcamp_db'\ntable_id = 'helinski_bike_trips'\ntable_full_id = f\"{client.project}.{dataset_id}.{table_id}\"\n\n# Write the DataFrame to BigQuery, schema should be inferred automatically from pyspark\ncleaned.write.format(\"bigquery\") \\\n    .option('table', table_full_id) \\\n    .option(\"writeMethod\", \"direct\") \\\n    .option(\"partitionField\", \"departure\")\\\n    .option(\"partitionType\", \"YEAR\")\\\n    .option(\"clusteredFields\", \"departure_id\")\\\n    .mode('overwrite') \\\n    .save()"}, {"cell_type": "code", "execution_count": null, "id": "7557f992-de57-4a54-b404-7225ebf678b9", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}